{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k4karthi/Indian-Sign-Language-to-Text-Conversion/blob/main/mobilenetv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries"
      ],
      "metadata": {
        "id": "9zQSBpXog1kd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDHxHMwVfSuP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AssemblyAI-Examples/mediapipe-python.git\n",
        "!pip install mediapipe\n",
        "!pip install PyQt5\n",
        "!pip install ipython==7.32.0\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9MosWEeqf17s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from base64 import b64encode\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import matplotlib as mpl\n",
        "import importlib\n",
        "import PyQt5\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "from keras.src import backend\n",
        "from keras.src import layers\n",
        "from keras.src.api_export import keras_export\n",
        "from keras.src.applications import imagenet_utils\n",
        "from keras.src.models import Functional\n",
        "from keras.src.ops import operation_utils\n",
        "from keras.src.utils import file_utils\n"
      ],
      "metadata": {
        "id": "x_VnmZV0t7YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Augmentation"
      ],
      "metadata": {
        "id": "76y_sSXyg-ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define augmentation pipeline\n",
        "augmentation = A.Compose([\n",
        "    A.Rotate(limit=5, p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.4),\n",
        "    A.HueSaturationValue(hue_shift_limit=2, sat_shift_limit=5, val_shift_limit=2, p=0.3),\n",
        "    A.MotionBlur(blur_limit=3, p=0.2),\n",
        "    A.GaussianBlur(blur_limit=(3, 3), p=0.1),\n",
        "    A.CLAHE(clip_limit=1.0, tile_grid_size=(8, 8), p=0.2),\n",
        "])\n",
        "\n",
        "# Set paths\n",
        "input_dir = \"path to input directory\"\n",
        "output_dir = \"path to output directory\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Randomly select n classes from the dataset. Change the 'n'\n",
        "\n",
        "n=10\n",
        "\n",
        "all_classes = [folder for folder in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, folder))]\n",
        "selected_classes = random.sample(all_classes, n)\n",
        "\n",
        "# Number of images per class . change as per your need\n",
        "target_count = 1000\n",
        "\n",
        "for class_folder in tqdm(selected_classes):\n",
        "    class_path = os.path.join(input_dir, class_folder)\n",
        "    output_class_path = os.path.join(output_dir, class_folder)\n",
        "    os.makedirs(output_class_path, exist_ok=True)\n",
        "\n",
        "    images = [img for img in os.listdir(class_path) if img.lower().endswith((\".jpg\", \".png\", \".PNG\"))]\n",
        "    original_count = len(images)\n",
        "\n",
        "    # Copy original images first\n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(os.path.join(output_class_path, img_name), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Augment until reaching 750 images\n",
        "    augment_needed = target_count - original_count\n",
        "    if augment_needed > 0:\n",
        "        for i in range(augment_needed):\n",
        "            img_name = random.choice(images)\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            augmented = augmentation(image=image)[\"image\"]\n",
        "            aug_img_name = f\"{img_name.split('.')[0]}_aug{i}.jpg\"\n",
        "            cv2.imwrite(os.path.join(output_class_path, aug_img_name), cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "print(f\"Augmentation completed for {n} classes with {target_count} images each.\")\n"
      ],
      "metadata": {
        "id": "wEETaQ84f2DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bounding Box Creation using Mediapipe"
      ],
      "metadata": {
        "id": "bQBKIoMY7diX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)"
      ],
      "metadata": {
        "id": "H0l2Gyssf2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'path to dataset'  # Your dataset path (15 gesture folders)\n",
        "img_size = 224  # Required size for MobileNetV2\n",
        "data = []\n",
        "labels = []\n",
        "classes = sorted(os.listdir(data_dir))\n"
      ],
      "metadata": {
        "id": "A3XZG-Axf2Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label in tqdm(classes):\n",
        "    if label == '.DS_Store':\n",
        "        continue\n",
        "    label_path = os.path.join(data_dir, label)\n",
        "    for img_name in os.listdir(label_path):\n",
        "        img_path = os.path.join(label_path, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = hands.process(img_rgb)\n",
        "\n",
        "        if results.multi_hand_landmarks:\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                h, w, _ = image.shape\n",
        "                x_vals = [lm.x * w for lm in hand_landmarks.landmark]\n",
        "                y_vals = [lm.y * h for lm in hand_landmarks.landmark]\n",
        "\n",
        "                x_min, x_max = int(min(x_vals)) - 20, int(max(x_vals)) + 20\n",
        "                y_min, y_max = int(min(y_vals)) - 20, int(max(y_vals)) + 20\n",
        "\n",
        "                x_min = max(0, x_min)\n",
        "                y_min = max(0, y_min)\n",
        "                x_max = min(w, x_max)\n",
        "                y_max = min(h, y_max)\n",
        "\n",
        "                hand_img = img_rgb[y_min:y_max, x_min:x_max]\n",
        "                if hand_img.size == 0:\n",
        "                    continue\n",
        "\n",
        "                hand_img = cv2.resize(hand_img, (img_size, img_size))\n",
        "                hand_img = preprocess_input(hand_img)  # MobileNetV2 preprocessing\n",
        "                data.append(hand_img)\n",
        "                labels.append(classes.index(label))\n"
      ],
      "metadata": {
        "id": "6XorHrM6f2JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Test Split"
      ],
      "metadata": {
        "id": "aikS5tzo7z80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(data)\n",
        "y = to_categorical(labels, num_classes=len(classes))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=22, shuffle=True)\n"
      ],
      "metadata": {
        "id": "qWpnksc5JiCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/processed_dataset\",\n",
        "    validation_split=0.15,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/processed_dataset\",\n",
        "    validation_split=0.15,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "fPbD267A1pQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Compilation & Training"
      ],
      "metadata": {
        "id": "9wwPTUbC73uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(img_size, img_size, 3),\n",
        "                                               include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze the base\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(len(classes), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6IkJ0maH1pTM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "X2LAQEGb1pVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Serialization"
      ],
      "metadata": {
        "id": "kte084Lu77Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mobilenetv2_handgesture_model.h5')\n"
      ],
      "metadata": {
        "id": "ebZZeN5N2GQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}